# -*- coding: utf-8 -*-
"""BBC-News-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OTlwCbwvjCI994UfyKN7aaOmG_k7KNm1

## Installing Libraries
"""

!pip install tensorflow_text

!pip install stopwords_guilannlp
!pip install hazm

"""## Import Libraries"""

# Commented out IPython magic to ensure Python compatibility.
# Read Data
import pandas as pd
import numpy as np


# Train
from sklearn.naive_bayes import MultinomialNB
import tensorflow_text as text
import tensorflow_hub as hub
import tensorflow as tf

# Preprocessing
from sklearn.model_selection import train_test_split
from stopwords_guilannlp import stopwords_output
from hazm import *
import re

# Visualization
from sklearn.metrics import accuracy_score , classification_report
from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt
# %matplotlib inline

"""## Loading Dataset"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
files=files.upload()

files = pd.read_json('bbcpersian.json')
files.head()

files.isnull().sum()

files.category.value_counts()

"""## Using 3 Categories"""

# Select balance data  from whole data
files = files.loc[files['category'].isin(['iran','sport','world'])]

"""## Preprocessing"""

puncs = ['،', '.', ',', ':', ';', '"',"'",'/','\\','_','-']

normalizer = Normalizer()

# Create custom stop_word
def load_stopwords():
    f = open("stopwords-fa.txt", "r", encoding='utf8')
    stopwords = f.read()
    stopwords = stopwords.split('\n')
    stopwords = set(stopwords)
    custom_stop_words = {'آنكه','آيا','بدين','براين','بنابر','میشه','میکنه','باشه','سلام','میکشه','اونی','و','در','از','چه'}
    stopwords = stopwords  | custom_stop_words
    stopwords = list(stopwords)[1:]
    unwanted_num = {'خوش','بهتر','بد','خوب','نیستم','عالی','نیست','فوق','بهترین','خیلی', 'نبود'}
    stopwords = [ele for ele in stopwords if ele not in unwanted_num]
    return stopwords


def clean_doc(doc):
    doc = normalizer.normalize(doc) # Normalize document using Hazm Normalizer
    tokenized = word_tokenize(doc)  # Tokenize text
    tokens = []
    for t in tokenized:
      temp = t
      for p in puncs:
        temp = temp.replace(p, '')
      tokens.append(temp)
    stop_set = load_stopwords()
    tokens = [w for w in tokens if not w in stop_set] # Remove stop words

    tokens = [w for w in tokens if not len(w) <= 1]  # Clean words
    tokens = [w for w in tokens if not w.isdigit()] #  Remove digits
    tokens =  [w for w in tokens if not re.match(r'[A-Z]+', w, re.I)] # Remove english letters
    tokens = ' '.join(tokens)
    return tokens

files.title = files['title'].apply(lambda x :clean_doc(x))

files.head()

# extract related_topics
related1 = files['related_topics'].apply(lambda x : x[0])
related2 = files['related_topics'].apply(lambda x : x[1])

# include related_topics with title
X = related1 + ' ' + related2 + ' ' + files.title


label_mapping = {'iran':0,'sport':1,'world':2}
files = files.copy()
files['category'] = files['category'].replace(label_mapping)

y = files['category'].copy()

X.shape , y.shape

from sklearn.feature_extraction.text import TfidfVectorizer

tf=TfidfVectorizer()
x=tf.fit_transform(X)

x.shape

X_train , X_test , y_train , y_test = train_test_split(x,y,test_size=0.1)

"""# Naive Bayes"""

nb = MultinomialNB()

# Model
nb_model = nb.fit(X_train, y_train)

# Predict
nb_predict = nb.predict(X_test)

# Accuracy
nb_acc = accuracy_score(y_test,nb_predict)

print('nb test accuracy:', nb_acc)

cr = classification_report(y_test, nb_predict)

print(cr)

"""# Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()

# Model
rf_model = rf.fit(X_train, y_train)

# Predict
rf_predict = rf.predict(X_test)

# Accuracy
rf_acc = accuracy_score(y_test,rf_predict)

print('rf test accuracy:', rf_acc)

cr = classification_report(y_test, rf_predict)

print(cr)

"""# Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier()
gbc.fit(X_train,y_train)

gbc_predict = gbc.predict(X_test)

gbc_acc = accuracy_score(y_test,gbc_predict)

print('gbc test accuracy:', gbc_acc)

cr = classification_report(y_test, gbc_predict)

print(cr)

"""# Support Vector Machine"""

from sklearn import svm

svm1 = svm.SVC()
svm1.fit(X_train,y_train)

svm1_predict = svm1.predict(X_test)

svm1_acc = accuracy_score(y_test,svm1_predict)

print('svm test accuracy:', svm1_acc)

cr = classification_report(y_test, svm_predict)

print(cr)

"""# KNeighbors"""

from sklearn.neighbors import KNeighborsClassifier

KNN =  KNeighborsClassifier()
KNN.fit(X_train,y_train)

KNN_predict = KNN.predict(X_test)

KNN_acc = accuracy_score(y_test,KNN_predict)

print('KNN test accuracy:', KNN_acc)

cr = classification_report(y_test, KNN_predict)

print(cr)

